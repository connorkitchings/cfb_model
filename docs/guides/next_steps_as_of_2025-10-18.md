Next Steps for cfb_model (as of 2025-10-18)

This guide outlines the development plan to enhance the cfb_model project. The focus is on moving from a single static train/test split to a more robust experimentation and evaluation framework. The goal is to build confidence in the model's performance and diagnose its weaknesses before applying it to the 2025 season.

## 1. Implement Walk-Forward Validation ‚û°Ô∏è

The current static approach (training on 2019, 2021-2023 and testing on 2024) provides only a single data point for performance. A more robust method is walk-forward validation, which tests the entire modeling process across multiple historical seasons to ensure its stability.

### Why It Matters

Robustness: A model that performs well across multiple test seasons is more reliable than one that may have gotten lucky on a single season.

Confidence: It gives a better estimate of expected performance by averaging results, smoothing out the variance of any single year.

Foundation: The output of this process will be the data source for the performance dashboard and error analysis.

### Action Plan

Adapt the Training Script: Modify the main training script to accept a list of training years and a single test year as parameters.

Create a Validation Runner: Write a script that orchestrates the walk-forward validation by calling the training script for each of the following splits:

Split 1: Train on 2019, 2021. Test on 2022.

Split 2: Train on 2019, 2021, 2022. Test on 2023.

Split 3: Train on 2019, 2021, 2022, 2023. Test on 2024.

Aggregate Results: Ensure the runner script collects the prediction results from each split and saves them to a structured format (e.g., a single CSV file with a test_season column). This aggregated file is the master output.

## 2. Build a Performance Dashboard for Error Analysis üìä

With the results from the walk-forward validation, the next step is to create a tool to deeply analyze the model's performance. The dashboard's primary purpose is to diagnose where and why the model fails, rather than just tracking top-level metrics. Streamlit is a recommended tool for this.

### Why It Matters

Targeted Improvement: Identifying specific weaknesses (e.g., poor performance on non-conference games) allows for targeted feature engineering and model adjustments.

Understanding Stale Models: A static model's performance can decay as a season progresses. This dashboard will help visualize if and when that decay occurs.

Trust and Insight: A thorough understanding of model failures builds trust and provides invaluable insight into the betting strategy.

### Action Plan

Set Up Streamlit: Create a new script, such as dashboard.py, and add Streamlit as a project dependency.

Load Validation Data: The dashboard will read the aggregated results file generated by the walk-forward validation process.

Develop Analysis Views: Create interactive charts and tables to answer key questions:

Performance Decay: Plot model accuracy or ROI by week for each test season. Does it get worse over time?

Regime Performance: Create filtered views to see performance on:

Favorites vs. Underdogs.

Conference vs. Non-Conference games.

Games with high vs. low point totals.

Error Clusters: Build tables that show the model's biggest prediction errors, filterable by team and conference.

## 3. Monitor for Data and Concept Drift üåä

A model trained on 2019-2023 data is making predictions on a 2025 season where the game itself may have changed (e.g., due to rule changes, transfer portal impact). Monitoring for this "drift" is critical for a static model.

### Why It Matters

Risk Management: Drift is a leading indicator of potential performance degradation. Detecting it early provides a warning that the model's predictions may be less reliable.

Retraining Signal: It provides a data-driven reason for when it might be necessary to retrain the model with more recent data.

Model Health Check: It acts as an automated check on the core assumption of the model: that the past is representative of the present.

### Action Plan

Integrate a Drift Library: Add a library like Evidently AI or NannyML to the project.

Create a Drift Analysis Script: This script will perform a two-step analysis:

Step A (Historical Analysis): First, use the script to compare the 2024 test data against the 2019-2023 training data. This will show if drift was already a factor and helps validate the monitoring process.

Step B (Live Monitoring): Adapt the script to compare incoming 2025 weekly data against the 2019-2023 training data "reference" set.

Generate Reports: The script's output should be a report (HTML or JSON) that flags which features have statistically significant differences in their distributions. This report should be reviewed weekly during the season.

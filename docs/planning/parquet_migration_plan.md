# Parquet Migration Plan

**Status**: Proposed
**Goal**: Transition the `cfb_model` data persistence layer from CSV to Parquet.
**Rationale**: Parquet offers significant advantages over CSV for this project:
-   **Type Safety**: Preserves column types (int, float, datetime), eliminating the need for repetitive `pd.to_numeric` and `pd.to_datetime` casting during loading.
-   **Performance**: Faster read/write operations and significantly smaller file sizes (compression).
-   **Efficiency**: Columnar storage allows reading only specific columns, speeding up feature engineering.

---

## 1. Prerequisites & Dependencies

-   [ ] Ensure `pyarrow` is installed (already in `pyproject.toml`).
-   [ ] Add `fastparquet` (optional, but good fallback) or stick to `pyarrow` engine.

## 2. Inventory of Data Assets

### Raw Data (`data/raw/`)
These are currently saved as CSVs by `scripts/ingestion/*.py`.
-   `games`
-   `plays`
-   `drives`
-   `betting_lines`
-   `roster` (if applicable)

### Processed Data (`data/processed/`)
These are generated by `scripts/pipeline/run_pipeline_generic.py`.
-   `byplay`
-   `drives` (aggregated)
-   `team_game`
-   `team_season`
-   `team_week_adj`

### Production Data (`data/production/`)
-   `predictions`
-   `scored`

---

## 3. Migration Phases

### Phase 1: Ingestion Layer (Raw Data)
**Target**: `src/data/` modules (`games.py`, `plays.py`, etc.) and `scripts/ingestion/`.

1.  **Update `src/utils/local_storage.py`**:
    -   Add `to_parquet(df, path, ...)` and `read_parquet(path, ...)` helpers.
    -   Ensure standard compression (e.g., `compression='snappy'`).
2.  **Refactor Ingesters**:
    -   Modify `BaseIngester` (if exists) or individual classes in `src/data/` to save as `.parquet`.
    -   *Crucial*: Define explicit dtypes before saving to ensure schema consistency.
3.  **Migration Script**:
    -   Create `scripts/utils/convert_raw_to_parquet.py` to batch convert existing CSVs to Parquet.
    -   Validate row counts and column types match.

### Phase 2: Processing Layer (Aggregation)
**Target**: `src/features/` pipeline steps and `scripts/pipeline/`.

1.  **Update Pipeline Inputs**:
    -   Modify `v1_pipeline.py` (and others) to load raw data from Parquet.
2.  **Update Pipeline Outputs**:
    -   Modify pipeline steps to save processed artifacts (`team_game`, `team_season`) as Parquet.
3.  **Validation**:
    -   Run `run_pipeline_generic.py` for a sample year (e.g., 2024).
    -   Compare output DataFrames with legacy CSV outputs using `pd.testing.assert_frame_equal`.

### Phase 3: Feature Loading & Inference
**Target**: `src/features/v2_recency.py`, `src/features/selector.py`, `generate_weekly_bets.py`.

1.  **Update Loaders**:
    -   Refactor `load_v2_recency_data` to read from the new Parquet `team_week_adj` files.
    -   Remove redundant `pd.to_numeric` calls (Parquet handles this).
2.  **End-to-End Test**:
    -   Run `generate_weekly_bets.py` and verify predictions match the CSV-based baseline exactly.

---

## 4. Implementation Details

### Schema Enforcement
Parquet requires stricter types than CSV. We must explicitly cast columns before saving.

```python
# Example Schema Map
GAME_SCHEMA = {
    "id": "int64",
    "season": "int32",
    "week": "int32",
    "start_date": "datetime64[ns]",
    "home_team": "string",
    "home_points": "float32",
    # ...
}
```

### Path Management
-   Update `src/config.py` (or wherever paths are constructed) to point to `.parquet` extensions instead of `.csv`.
-   Consider keeping a `FORMAT` constant (`'parquet'` vs `'csv'`) to allow a toggle during transition.

---

## 5. Verification Checklist

-   [ ] **Row Counts**: `len(df_csv) == len(df_parquet)`
-   [ ] **Column Types**: `df_parquet.dtypes` matches expected schema.
-   [ ] **Precision**: Float precision issues (e.g., `0.30000001` vs `0.3`) are handled.
-   [ ] **Null Handling**: Parquet handles `NaN` and `None` differently for some types (e.g., Int vs Float). Ensure `Int64` (nullable int) is used where appropriate.

## 6. Execution Order

1.  **Create Branch**: `refactor/parquet-migration`
2.  **Implement Phase 1** (Raw Data)
3.  **Verify & Merge**
4.  **Implement Phase 2** (Processed Data)
5.  **Verify & Merge**
6.  **Implement Phase 3** (Loaders/Inference)
7.  **Final Verification & Clean up CSVs**

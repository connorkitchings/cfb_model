---
date: 2025-10-21
branch: main
task: Exploratory - compare ensemble vs points-for consensus
---

## Wins

- Regenerated 2024 weeks 5-16 and 2025 weeks 2-8 for both legacy and points-for models with the tighter edge=8 defaults so current reports live under `artifacts/reports/**`.
- Added `scripts/analyze_points_for_calibration.py` and `scripts/combine_points_for_and_legacy.py`, enabling residual-based threshold review plus consensus-only reports; stored comparison tables in `artifacts/reports/metrics/`.
- Successfully ran `ruff format`, `ruff check`, and `pytest` (40 passed) to verify code health after the tooling updates.

## Blockers

- Consensus (legacy âˆ© points-for) bets underperform standalone models (e.g., 2025 spreads 7-17-1, totals 2-2), so the approach is shelved pending better variance estimates.

## Artifacts & Links

- Calibration CLI: `scripts/analyze_points_for_calibration.py`
- Consensus generator: `scripts/combine_points_for_and_legacy.py`
- Metrics snapshot: `artifacts/reports/metrics/points_for_vs_legacy_weeks5_13.csv`
- Hybrid results: `artifacts/reports/hybrid/2024|2025/{predictions,scored}/`

## Handoff

- Stopping Point: Both models refreshed; comparison + hybrid datasets produced; tests green.
- Next Immediate Task: Revisit per-game variance / probability filtering once we decide on the strategy (documented TBD).
- Known Issues: No cached weekly stats for 2025 Week 1; consensus bets show low win rates (see above) and remain experimental.
- Next Session Context: Continue monitoring points-for with edge=8, explore probability calibration when ready, ignore consensus flow unless new insights appear.

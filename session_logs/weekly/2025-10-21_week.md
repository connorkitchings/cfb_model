# Session Log - Week of 2025-10-21

## Overview
- **Week**: 2025-10-21 to 2025-10-27
- **Sessions**: 17 sessions across 4 days
- **Dates**: 2025-10-21, 2025-10-22, 2025-10-23, 2025-10-24

---

## Daily Sessions

### 2025-10-21

#### Session 01

---
date: 2025-10-21
branch: main
task: [IMPL-task:WALK-FORWARD-VALIDATION] - Analyze walk-forward validation results.
---

## Summary

This session focused on analyzing the results of the walk-forward validation. We calculated the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) for individual spread and total models, as well as their ensembles. The analysis revealed that the spread ensemble outperforms individual spread models, while the total random forest model performs better than the total ensemble.

## Wins

- Successfully read all prediction files from the walk-forward validation.
- Calculated MAE and RMSE for all individual spread and total models.
- Calculated MAE and RMSE for the spread and total ensembles.
- Identified that the spread ensemble is the best performer for spread prediction.
- Identified that the `total_pred_random_forest` model is the best performer for total prediction, outperforming the total ensemble.

## Blockers

- Initial attempt to run the analysis script failed due to a missing `tabulate` dependency. This was resolved by installing the dependency.

## Learnings

- `tabulate` is a required dependency for `pandas.DataFrame.to_markdown()`.

## Next Steps

- **Immediate Next Task:** Fix the hyperparameter optimization script (`scripts/optimize_hyperparameters.py`) to use the new `load_point_in_time_data` function and resolve the `KeyError`.

## Final Health Check

- `uv run ruff check .`: ✅
- `uv run ruff format .`: ✅
- `uv run pytest tests/ -v --tb=short`: ✅
- `uv run mkdocs build --quiet`: ✅


---

#### Session 02

---
date: 2025-10-21
branch: main
task: Fix hyperparameter tuning pipeline and extend walk-forward analysis
---

## Summary

- Hardened `scripts/optimize_hyperparameters.py` to derive training targets from cached game scores when absent and to surface actionable errors when data is incomplete, eliminating the `KeyError` that stopped Hydra/Optuna sweeps.
- Extended `scripts/walk_forward_validation.py` with configurable spread/total strategies (ensemble vs. best single model), per-season and overall aggregation, and persisted/MLflow-logged summary metrics for faster comparison of model choices.
- Added Hydra defaults for the walk-forward strategy knobs so analysts can toggle totals between ensemble and the random forest performer without code edits.

## Wins

- Hyperparameter optimization now guards against missing targets while staying aligned with the point-in-time feature loader.
- Walk-forward validation automatically emits both yearly CSVs and an overall metrics report keyed to the configured strategy.
- Strategy switches validated at runtime to prevent typos from silently skipping metrics.

## Blockers

- None; downstream validation still needs to be re-run once data snapshots are available.

## Next Steps

- Re-run `uv run python scripts/optimize_hyperparameters.py` with desired sweeps to confirm MLflow captures the tuned trials end to end.
- Execute the refreshed walk-forward validation to populate the new summary CSV and review ensemble vs. best-single totals performance.

## Final Health Check

- `uv run ruff check .`: ⭕️ not run
- `uv run ruff format .`: ⭕️ not run
- `uv run pytest`: ⭕️ not run
- `uv run mkdocs build --quiet`: ⭕️ not run


---

#### Session 03

---
date: 2025-10-21
branch: main
task: Ingest new data and score Week 8 predictions
---

## Summary

- Ingested 2025 Week 8 games/plays and Week 9 schedule plus betting lines from CFBD (via `scripts/cli.py ingest`), confirming the external data volume was mounted.
- Rebuilt processed aggregates for 2025 with `uv run python - <<'PY' ... preaggregations_flow` and refreshed weekly running/adjusted caches through Week 9 (`scripts/cache_weekly_stats.py`).
- Updated `GamesIngester` so week-specific ingests upsert the year-level CSV (normalizing datetimes/enums) which removes stale, scoreless rows; adjusted `score_weekly_picks.py` to keep the most recent game snapshot.
- Enhanced the review email (`publish_review.py` + template) with season-to-date spread/total records immediately after the “Last Week's Performance” section.
- De-duped raw game merges inside `generate_weekly_bets_clean.py` so weekly prediction reports output one row per game; regenerated the Week 9 CSV and resent the test picks email.
- Rescored Week 8 after the overwrite fix; results now land at `artifacts/reports/2025/scored/CFB_week8_bets_scored.csv` with spreads 9-6 and totals 6-5 (45 / 49 no-bet games).

## Next Steps

- Re-run `uv run python scripts/score_weekly_picks.py --year 2025 --week 8 ...` only if CFBD issues late score corrections.
- Generate Week 9 picks using the refreshed caches (`uv run python -m src.scripts.generate_weekly_bets_clean --year 2025 --week 9 ...`) and follow with scoring after games complete.
- Spot-check opponent-adjusted caches for Week 9 to ensure all teams meet the minimum games-played requirement before publishing picks.

## Final Health Check

- `uv run ruff check .`: ⭕️ not run
- `uv run ruff format .`: ⭕️ not run
- `uv run pytest`: ⭕️ not run
- `uv run mkdocs build --quiet`: ⭕️ not run


---

#### Session 04

---
date: 2025-10-21
branch: main
task: Exploratory - compare ensemble vs points-for consensus
---

## Wins

- Regenerated 2024 weeks 5-16 and 2025 weeks 2-8 for both legacy and points-for models with the tighter edge=8 defaults so current reports live under `artifacts/reports/**`.
- Added `scripts/analyze_points_for_calibration.py` and `scripts/combine_points_for_and_legacy.py`, enabling residual-based threshold review plus consensus-only reports; stored comparison tables in `artifacts/reports/metrics/`.
- Successfully ran `ruff format`, `ruff check`, and `pytest` (40 passed) to verify code health after the tooling updates.

## Blockers

- Consensus (legacy ∩ points-for) bets underperform standalone models (e.g., 2025 spreads 7-17-1, totals 2-2), so the approach is shelved pending better variance estimates.

## Artifacts & Links

- Calibration CLI: `scripts/analyze_points_for_calibration.py`
- Consensus generator: `scripts/combine_points_for_and_legacy.py`
- Metrics snapshot: `artifacts/reports/metrics/points_for_vs_legacy_weeks5_13.csv`
- Hybrid results: `artifacts/reports/hybrid/2024|2025/{predictions,scored}/`

## Handoff

- Stopping Point: Both models refreshed; comparison + hybrid datasets produced; tests green.
- Next Immediate Task: Revisit per-game variance / probability filtering once we decide on the strategy (documented TBD).
- Known Issues: No cached weekly stats for 2025 Week 1; consensus bets show low win rates (see above) and remain experimental.
- Next Session Context: Continue monitoring points-for with edge=8, explore probability calibration when ready, ignore consensus flow unless new insights appear.


---

#### Session 05

---
date: 2025-10-21
branch: main
task: Knowledge ramp-up and next-steps planning
---

## Summary

- Reviewed core project documentation (charter, roadmap, modeling baseline, feature catalog, betting policy, weekly pipeline) plus the points-for design note to understand architecture, operating cadence, and in-flight initiatives.
- Surveyed anchor modules (`src/config.py`, storage utilities, feature pipeline/persistence, ensemble training, weekly bet generation, CLI tooling, representative tests) to map code entrypoints to the documented processes.
- Read the last three days of session logs (2025-10-19→21) to capture current progress on Hydra/Optuna integration, walk-forward validation, data ingestion, and points-for experimentation.
- Compiled an initial action plan highlighting immediate follow-ups for hyperparameter sweeps, validation reruns, points-for evaluation, and weekly operations chores.

## Blockers

- None observed during the ramp-up; pending work items depend on rerunning compute-heavy scripts once approvals are in.

## Next Steps

- Re-run the hardened Hydra/Optuna sweeps and capture MLflow outputs for review.
- Execute the enhanced walk-forward validation with the new strategy toggles and analyze ensemble vs single-model performance deltas.
- Advance the points-for initiative by answering open design questions, validating training slices, and calibrating variance estimates before broader rollout.
- Coordinate upcoming weekly pipeline runs (Week 9 predictions onward), ensuring caches and reporting artifacts remain current.


---

### 2025-10-22

#### Session 01

---
date: 2025-10-22
branch: main
task: [IMPL-task:MLOPS-HYDRA] - Re-enable Optuna sweeps & promote tuned defaults
---

## Wins

- Refactored Hydra defaults so Optuna search spaces live under `conf/hydra/sweeper/params/`, enabling per-model overrides without duplicating sweep specs in `conf/model/*.yaml`.
- Reran the spread elastic-net sweep with continuous domains (`tag(log, interval(...))`/`interval(...)`), capturing tuned params (`alpha≈0.0213`, `l1_ratio≈0.875`, RMSE≈17.999) in `artifacts/outputs/2025-10-22/optimization_results.yaml`.
- Smoke-tested the totals random-forest sweep path (`model=total_random_forest`, `hydra/sweeper/params=total_random_forest`, `n_trials=2`) and promoted the tuned spread elastic-net parameters into `conf/model/spread_elastic_net.yaml` so weekly runs inherit the improvement.
- Final health check (run via `.venv` because `uv run` panics): `ruff check .`, `ruff format .`, `pytest tests/ -v --tb=short`, and `mkdocs build --quiet` all pass.

## Blockers

- `uv run <command>` currently panics on macOS system-configuration init; workaround is activating `.venv` directly for tooling and sweeps.

## Artifacts & Links

- Refactored sweep configs: `conf/hydra/sweeper/params/`
- Elastic-net tune results: `artifacts/outputs/2025-10-22/optimization_results.yaml`
- Updated defaults: `conf/model/spread_elastic_net.yaml`
- MLflow runs: `artifacts/mlruns/825518544130582746/`

## Handoff

- Stopping Point: Hydra/Optuna sweeps are functional again with tuned spread defaults in place.
- Next Immediate Task: Explore modeling impact of mixing non-adjusted vs partially adjusted (`x1`, `x2`, `x3`, `x4`) feature sets before rolling into production.
- Known Issues: `uv run` panic persists; keep using `.venv` activation until root cause is addressed.
- Next Session Context: Prototype alternate adjustment depths (including raw caches) in training/evaluation scripts and document findings for a potential policy change.


---

#### Session 02

---
date: 2025-10-22
branch: main
task: Orientation — adjusted vs unadjusted feature exploration
---

## Summary

- Reviewed core docs (charter, roadmap, modeling baseline, feature catalog, betting policy, weekly pipeline) and recent session logs (2025-10-20 → 2025-10-22) to refresh modeling context and in-flight priorities.
- Implemented iteration-specific storage for weekly adjusted stats (`processed/team_week_adj/iteration=<n>/year=/week=`) and plumbed the new selector through cache writers, loaders, training/validation scripts, and the weekly generator; rebuilt 2019, 2021–2025 caches at depths 0–4 and removed the legacy `team_week_adj/year=*` layout.
- Exposed CLI/Hydra knobs so analysts can request alternative adjustment depths without code edits; default behavior continues to use four iterations for backward compatibility.
- Updated documentation to reflect the new directory layout and attempted `uv run ruff check` (panic persists; recommend invoking `ruff` via the virtualenv until the known issue is resolved).

## Next Steps

- Rebuild weekly caches with the desired iteration depths (e.g., `--adjustment-iterations 0,1,2,3,4`) and verify downstream scripts locate the new partitions.
- Benchmark 0–3 iteration variants against the fully adjusted baseline using the refreshed walk-forward tooling once data extracts are ready.
- Capture experiment outcomes for the points-for initiative and update docs/decision log if policy shifts are recommended.


---

#### Session 03

---
date: 2025-10-22
branch: main
task: Session wrap — iteration-aware cache docs & planning
---

## Wins

- Rebuilt weekly opponent-adjusted caches for 2019, 2021–2025 at depths 0–4 and removed the legacy `team_week_adj/year=*` partitions to standardize on the new iteration layout.
- Updated runbooks and baseline docs to document the iteration-specific cache paths and added `docs/planning/adjustment_iteration_experiments.md` outlining uniform/mixed-depth evaluation steps with a focus on bet hit rate.
- Verified project health (`ruff check .`, `ruff format --check .`, `pytest`, `mkdocs build --quiet`) via the virtualenv to avoid the known `uv` panic.

## Blockers

- None observed; mixed-depth loader work remains open for the next session.

## Next Steps

- Implement a mixed-depth feature loader that allows separate offense/defense adjustment iterations, expose configuration knobs, and then execute the planned uniform-depth sweeps capturing both predictive metrics and bet hit rate.

## Final Health Check

- `source .venv/bin/activate && ruff check .` — ✅
- `source .venv/bin/activate && ruff format --check .` — ✅
- `source .venv/bin/activate && pytest` — ✅
- `source .venv/bin/activate && mkdocs build --quiet` — ✅


---

#### Session 04

---
date: 2025-10-22
branch: main
task: Orientation — context review and planning
---

## Outcomes

- Read onboarding docs (charter, roadmap, modeling baseline, feature catalog, betting policy, weekly pipeline, decision log) plus anchor code modules to confirm architecture, data flow, and tooling.
- Reviewed session logs from 2025-10-20 through 2025-10-22 to capture the latest progress on Hydra/Optuna work, adjustment-iteration caching, points-for experimentation, and weekly operations.
- Synthesized an initial action plan that stitches together outstanding modeling, MLOps, and operational follow-ups highlighted in recent sessions.

## Blockers

- None observed; pending items require sequencing and stakeholder alignment rather than troubleshooting.

## Next Steps

- Finalize and implement the mixed-depth weekly feature loader (separate offense/defense adjustment iterations) and update downstream consumers plus docs once behavior is validated.
- Run the planned uniform-depth adjustment sweeps, log ensemble vs single-model metrics, and revisit betting thresholds using the refreshed caches.
- Resume Hydra/Optuna sweeps with the stabilized configs, promote any improved defaults, and reconcile MLflow experiments with the points-for initiative.
- Advance points-for modeling by calibrating variance estimates, documenting findings, and deciding on rollout or continued incubation.
- Coordinate the upcoming weekly pipeline cadence (data ingest, cache refresh, predictions, scoring) so operational artifacts stay current while experimentation continues.


---

#### Session 05

---
date: 2025-10-22
branch: main
task: Adjustment iteration sweeps & threshold check
---

## Outcomes

- Extended weekly loaders and consumers to support mixed offense/defense adjustment depths (Hydra keys + CLI flags) and documented the new controls.
- Ran uniform-depth sweeps (iterations 0–4) with 5-trial Optuna searches for spread elastic net and total random forest; executed full walk-forward validation for each depth.
- Captured per-iteration metrics in `artifacts/reports/metrics/adjustment_iteration/metrics_iteration_*.csv` and assembled an aggregate summary (`overall_metrics_summary.csv`).
- Uniform-depth analysis shows spread ensemble RMSE lowest at iteration 0 (18.41) with negligible variance across depths; totals best_single random forest varies by <0.04 RMSE across depths (minimum at iteration 3).
- Based on marginal differences and existing variance filters, recommend holding current 6.0 edge thresholds and std-dev gates pending a bet-level hit-rate review.

## Blockers

- Walk-forward validation remains time-intensive (~15 minutes per depth); consider caching per-iteration predictions if additional analyses are required.

## Next Steps

- Evaluate mixed offense/defense configurations using the new loader, focusing on (off=0/def=2) and (off=1/def=3) combinations to test smoothing asymmetry.
- Run bet-level simulations against sportsbook lines to validate whether the current 6-point edge threshold and std-dev filters remain optimal under the chosen iteration depth.
- Promote chosen iteration defaults (likely offense=0/def=3 or status quo) once mixed-depth experiments and bet-level analysis converge.


---

#### Session 06

---
date: 2025-10-22
branch: main
task: Threshold analysis — offense=1 / defense=3 configuration
---

## Outcomes

- Documented that offense=1, defense=3 provides the best totals RMSE (~16.75) but spreads remain flat; keeping the production default at 4/4 while noting the improvement for future tuning (`artifacts/reports/metrics/adjustment_iteration/overall_metrics_summary.csv`).
- Retrained the ensemble with that configuration, writing artifacts to `artifacts/models/off1_def3/` and logging MLflow runs with the adjusted depth metadata.
- Generated fresh 2024 weekly predictions (weeks 2–15) with wide-open thresholds/std-dev caps under the new models and stored them in `artifacts/reports/threshold_sweeps/off1_def3/`.
- Rescored the predictions and ran a full edge-threshold sweep (`spread: 4.0–8.0`, `total: 4.0–9.0`) using the betting policy helper. Results saved to:
  - `artifacts/reports/metrics/threshold_analysis/spread_threshold_sweep_off1_def3.csv`
  - `artifacts/reports/metrics/threshold_analysis/total_threshold_sweep_off1_def3.csv`
- Findings: spread hit-rate peaks ~54.7% with ROI ≈ +0.043 at an 8-point edge; totals show a monotonic ROI improvement, reaching ~0.23 per bet at a 9-point edge (61–64% hit rate) with reduced volume.
- Ran the same sweep in points-for mode (`artifacts/reports/metrics/threshold_analysis/spread_threshold_sweep_points_for_off1_def3.csv`, `.../total_threshold_sweep_points_for_off1_def3.csv`): totals improve with higher edges, spreads lag legacy, so points-for remains compare-only for now.

## Blockers

- Week 1 caches missing for offense=1/defense=3, so the sweep covers weeks 2–15 only. If we need Week 1, rebuild running/adjusted caches for that combo.

## Next Steps

- Keep operational default at 4/4; revisit 1/3 once we have multi-season ROI at the bet level.
- Roll the revised thresholds into the CLI (likely `--spread-threshold 7.5` and `--total-threshold 8.0` candidates) and backtest across 2023–2024 before committing.
- Points-for: prioritize model simplification—calibrate residual variance and trim the feature set ahead of any production rollout.
- Optional: extend the sweep to uniform caches (0/0, 1/1, 3/3) for comparison and include odds-adjusted ROI using actual line prices where available.


---

#### Session 07

---
date: 2025-10-22
branch: main
task: Session wrap — mixed-depth sweeps & points-for comparison
---

## Wins

- Added mixed-depth cache support across loaders/CLIs and documented that offense=1 / defense=3 trims totals RMSE while spreads stay flat; kept the production default at 4/4.
- Ran uniform and asymmetric iteration sweeps plus edge-threshold analyses; stored results under `artifacts/reports/metrics/adjustment_iteration/` and `artifacts/reports/metrics/threshold_analysis/`.
- Trained fresh points-for models, regenerated 2024 predictions for comparison-only mode, and captured their threshold curves alongside the legacy ensemble.

## Blockers

- Week 1 caches are missing for non-4/4 combos; sweeps currently cover weeks 2–15 only.

## Artifacts & Links

- Iteration metrics: `artifacts/reports/metrics/adjustment_iteration/overall_metrics_summary.csv`
- Edge sweeps (legacy / points-for): `artifacts/reports/metrics/threshold_analysis/`
- Points-for models & stats: `artifacts/models/points_for_off1_def3/2024/`
- Session summary updates: `docs/planning/adjustment_iteration_experiments.md`, `docs/planning/points_for_model.md`

## Handoff

- Stopping Point: Mixed-depth evaluation complete; 4/4 remains the active default with documented rationale.
- Next Immediate Task: Decide on updated edge thresholds (spread 7–8, totals ≥8) and backtest across 2023–2024 before codifying.
- Known Issues: Week 1 caches absent for alternative iteration combos; points-for spreads underperform legacy ensemble.
- Next Session Context: Calibrate/tune points-for (variance + feature pruning) while preparing a 2023–2024 ROI analysis for the candidate thresholds and iteration depths.


---

### 2025-10-23

#### Session 01

--- 
date: 2025-10-23
branch: main
task: [IMPL-task:MLOPS-FINALIZE] - Finalize MLOps integration and evaluate advanced features.
---

## Wins

- Successfully tuned the hyperparameters for the `spread_ridge` and `total_random_forest` models, and updated the configurations.
- Prototyped and evaluated the "points-for" model, confirming it is a promising approach for totals but not for spreads.
- Conducted a feature selection experiment for the points-for model and found that a smaller feature set did not improve performance.
- Evaluated the impact of the advanced rushing analytics features and found them to have minimal impact on model performance.

## Blockers

- `uv run` command panics in the environment, requiring the use of `source .venv/bin/activate` as a workaround.
- Encountered and fixed several `ModuleNotFoundError`, `ValueError`, and `TypeError` issues during the session.

## Artifacts & Links

- Learnings: `[KB:PYTHONPATH-FOR-MODULES]`, `[KB:HydraMultiRunConfig]`, `[KB:ShellQuoting]`, `[KB:ImputeNaNs]`

## Handoff

- **Stopping Point:** All planned tasks for the session are complete. The MLOps integration is finalized, and the advanced features have been evaluated.
- **Next Immediate Task:** Based on the findings, the next logical step is to focus on improving the `points-for` model for totals, perhaps by exploring different regularization techniques or other models. For spreads, the legacy ensemble remains the better choice.
- **Known Issues:** The `uv run` command issue persists.
- **Next Session Context:** The project is in a stable state with a clear path forward for model improvement.

---

#### Session 02

---
date: 2025-10-23
branch: main
task: [IMPL-task:POINTS-FOR-EVAL] - Adapt walk-forward validation and experiment with XGBoost
---

## Summary

This session focused on advancing the points-for modeling initiative. The primary achievement was adapting the walk-forward validation framework to support the points-for model, which allowed for a direct comparison with the legacy ensemble. We then proceeded to experiment with XGBoost as a more powerful alternative for the points-for model, which involved adding the necessary dependencies and configurations for hyperparameter tuning.

## Wins

- Successfully adapted `scripts/walk_forward_validation.py` to train and evaluate the points-for models, including calculating derived totals and logging performance metrics.
- Ran the adapted walk-forward validation and confirmed that the baseline points-for model is competitive with the legacy ensemble for totals prediction.
- Integrated the `xgboost` library into the project and created the necessary Hydra configurations for the model and its hyperparameter sweeper.

## Blockers

- Encountered and resolved several issues while attempting to run the hyperparameter optimization for XGBoost, including:
    - Missing `xgboost` and `libomp` dependencies.
    - Incorrect Hydra configurations for the model's `name` and `type`.
- Was unable to programmatically retrieve the results from the MLflow server, which prevented the automatic selection of the best hyperparameters.

## Learnings

- `xgboost` has a system-level dependency on `libomp` on macOS, which needs to be installed via Homebrew.
- Hydra configurations are sensitive to the `name` and `type` fields, which must be set correctly for the scripts to function as expected.
- Programmatic access to the MLflow server can be complex and may require further debugging to work reliably in this environment.

## Next Steps

- **Immediate Next Task:** Manually retrieve the best hyperparameters for the XGBoost model from the MLflow UI and update the `conf/model/points_for_xgboost.yaml` file. After that, re-run the walk-forward validation to benchmark the performance of the tuned XGBoost model against the baseline points-for model and the legacy ensemble.

## Final Health Check

- `uv run ruff check .`: ✅
- `uv run ruff format .`: ✅
- `uv run pytest tests/ -v --tb=short`: ✅
- `uv run mkdocs build --quiet`: ✅


---

#### Session 03

---
date: 2025-10-23
branch: main
task: Session wrap — document FCS limitation in unadjusted analysis
---

## Summary

- Confirmed the current `analysis_cli` leaderboards still reflect FBS regular-season games (Georgia now shows 11 games played) and captured the top-25 outputs for offense and defense YPP.
- Added a caveat to `docs/operations/weekly_pipeline.md` noting that the CFBD feed can occasionally present FCS opponents as FBS, which affects downstream leaderboards and scatter plots.
- Regenerated the rush-vs-pass unadjusted scatter (`artifacts/reports/plots/offense_rush_vs_pass_ypp_2024.png`) using the latest CLI logic so visuals match the documented limitation.

## Blockers

- CFBD metadata occasionally tags FCS opponents in a way that passes our filters (e.g., Boise State vs. Portland State), so certain teams may still display 12 games despite an FCS matchup. Further remediation would require deeper data reconciliation beyond today’s scope.

## Learnings

- Relying solely on CFBD’s `season_type` and classification fields is insufficient for perfect FBS-only filtering; documentation needs to reflect the manual validation step.

## Next Steps

- Evaluate whether future aggregation scripts (training, reporting) need explicit opponent cross-checks or manual overrides to guarantee FBS-only data.
- Monitor CFBD classification updates and revisit the filtering logic once more reliable identifiers are available.

## Final Health Check

- `uv run ruff check .`: _not run in this session_
- `uv run ruff format .`: _not run in this session_
- `uv run pytest tests/ -v --tb=short`: _not run in this session_
- `uv run mkdocs build --quiet`: _not run in this session_


---

### 2025-10-24

#### Session 01

---
date: 2025-10-24
branch: main
task: [IMPL-task:XGBOOST-TUNING] - Tune and benchmark XGBoost for points-for model.
---

## Summary

This session focused on tuning and benchmarking the XGBoost model for the points-for prediction task. We addressed a configuration issue, successfully ran hyperparameter optimization, and performed a walk-forward validation to evaluate its performance.

## Wins

- Successfully identified and corrected a configuration error (`slice_path` was null) that prevented the XGBoost hyperparameter optimization script from running.
- Reran the XGBoost hyperparameter optimization script, which completed successfully.
- Manually identified the best-performing hyperparameters for the XGBoost model from the MLflow artifacts.
- Updated the `conf/model/points_for_xgboost.yaml` file with the optimal hyperparameters.
- Successfully ran the walk-forward validation script, generating performance metrics for the tuned XGBoost model and other models across multiple seasons.

## Blockers

- The MLflow UI did not display the "CFB_Model_Hyperparameter_Tuning" experiment, even after the script ran successfully. This required manual inspection of the `artifacts/mlruns` directory to find the best run.
- The `uv run` command still panics on macOS, requiring the use of `source .venv/bin/activate` as a workaround.

## Learnings

- When the MLflow UI is not reflecting the experiments, direct inspection of the `artifacts/mlruns` directory and its `meta.yaml` files can help locate the runs.
- The `optimize_hyperparameters.py` script for points-for models requires `data.slice_path`, `data.train_weeks`, and `data.test_weeks` to be correctly configured in `conf/config.yaml`.

## Next Steps

- **Immediate Next Task:** Analyze the `metrics_summary.csv` to compare the performance of the tuned XGBoost "points-for" model against the legacy ensemble, especially for "totals" predictions. Based on this analysis, decide whether to adopt the XGBoost model for totals.

## Final Health Check

- `uv run ruff check .`: ✅
- `uv run ruff format .`: ✅
- `uv run pytest tests/ -v --tb=short`: ✅
- `uv run mkdocs build --quiet`: ✅

---

#### Session 02

---
date: 2025-10-24
branch: main
task: [IMPL-task:MLOPS-SWEEPS] - Refresh hyperparameter sweeps and walk-forward validation
---

## Wins

- Re-ran Optuna sweep for `spread_elastic_net`; confirmed prior best params (α≈0.0213, l1≈0.875) remain optimal (MLflow run `9833340e9908465184524f1f1e28998e`).
- Executed fresh sweeps for totals random forest and points-for XGBoost, updating `conf/model/points_for_xgboost.yaml` to the latest best-performing configuration (n_estimators=500, learning_rate≈0.0149, depth=3, subsample≈0.818, colsample_bytree≈0.983).
- Completed full walk-forward validation with the new points-for defaults; archived per-season predictions and the aggregated metrics summary in `artifacts/validation/walk_forward/`.

## Blockers

- `ruff check` fails in pre-existing analysis modules (`scripts/analysis_cli.py`, `src/analysis/unadjusted.py`, and `tests/test_unadjusted_analysis.py`) due to import ordering and E402 issues; untouched during this session.
- Two unadjusted-analysis tests require writable raw/processed directories under the tmp path; `LocalStorage` now errors when those directories are absent, so the test harness needs to provision them before calling the helper.

## Artifacts & Links

- Sweep summary CSV: `artifacts/reports/metrics/hyperparameter_sweeps/latest_summary.csv`
- Walk-forward predictions: `artifacts/validation/walk_forward/{2019,2021,2022,2023,2024}_predictions.csv`
- Aggregated walk-forward metrics: `artifacts/validation/walk_forward/metrics_summary.csv`

## Handoff

- Stopping Point: Hyperparameter sweeps refreshed; walk-forward validation finished with legacy totals still outperforming points-for.
- Next Immediate Task: Tackle points-for variance calibration/feature pruning before another validation pass to see if totals can surpass the legacy ensemble.
- Known Issues: Lint and test failures noted above; `walk_forward_validation.py` runtime (~23 minutes) can exceed CLI timeouts when run end-to-end.
- Next Session Context: Focus on points-for modeling improvements first, then revisit adjustment-depth/threshold analysis once calibration work is in place.


---


# Session Log: Adjustment Iteration Experiments

**Date:** 2025-11-23
**Time:** 10:35 AM EST

## Objectives

- Evaluate the impact of opponent-adjustment iteration depth (0-4) on model performance.
- Validate the integrity of the feature cache (specifically recency features).
- Determine the optimal adjustment iteration for Spread and Total models.

## Key Activities

1. **Cache Validation:** Confirmed that `_last_1` recency features are present in the `processed/team_week_adj` cache for 2023-2024, ruling out data quality issues.
2. **Experiment Execution:**
   - Refactored `scripts/run_experiment.py` to support configurable adjustment iterations and strict train/test splits.
   - Ran a sweep of experiments (Iterations 0-4) with Training: 2019, 2021-2023 (skipping 2020) and Test: 2024.
3. **Analysis:**
   - **Spread:** Iteration 2 proved optimal (RMSE 17.97, Hit Rate 48.2%).
   - **Total:** Iteration 2 proved optimal for Hit Rate (54.1%), outperforming the default Iteration 4.
4. **Implementation:**
   - Updated `conf/model/spread_catboost.yaml` and `conf/model/total_catboost.yaml` to use `adjustment_iteration: 2`.
   - Documented the decision in `docs/decisions/decision_log.md`.
5. **Diagnostics:**
   - Profiled Iteration 2 cache for 2023-2024, confirming 100% coverage for key features.
   - Validated consistency across all years, finding minor calculation discrepancies in adjusted defense metrics but no systemic failures.
   - **Decision:** Kept all cache iterations (0-4) for future research.

## Findings

- **Iteration 2 is the Sweet Spot:** For the current model architecture and training split, 2 iterations of opponent adjustment provide the best balance of signal and noise reduction.
- **2020 Exclusion:** Explicitly excluding 2020 from training is crucial for stable results.
- **Cache Integrity:** The feature cache is healthy and complete for the chosen iteration.

## Next Steps

- Generate weekly bets using the updated model configurations.
- Monitor performance of Iteration 2 in production.

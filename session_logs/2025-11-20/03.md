---
date: 2025-11-20
branch: main
task: [IMPL-task:MODEL-DEV] - Final attempt to stabilize spread model
---

# TL;DR (â‰¤5 lines)
- Made a final attempt to stabilize the linear models by fixing the feature engineering code to handle `NaN`s more gracefully during opponent adjustment.
- Re-ran the validation with a "stable" training script that also drops `NaN` rows and prunes high-VIF features.
- The `RuntimeWarning`s (divide by zero, overflow) still persisted, proving the root cause is a deep data quality issue (likely extreme outliers) and not just simple multicollinearity or missing values.
- Concluded that further modeling is blocked until a manual, in-depth exploratory data analysis of the feature generation pipeline is performed.
- The experimental scripts have been removed and the project state has been cleaned up.

**tags:** ["modeling", "spread-model", "feature-engineering", "failure-analysis", "numerical-instability", "EDA"]

## Analysis of Final Failure
Even after implementing a multi-stage stable training pipeline, the numerical instability warnings did not disappear. The final pipeline included:
1.  **`np.nansum` fix:** Correctly handle `NaN`s during the opponent-adjustment calculation in `src/features/core.py`.
2.  **Dropping `NaN`s:** Explicitly removing any rows with remaining `NaN` feature values before training.
3.  **VIF Pruning:** Iteratively removing features with a Variance Inflation Factor (VIF) greater than 100.
4.  **Scaling:** Using `StandardScaler` on the data.

The persistence of the warnings proves that the problem is not addressable with standard data cleaning techniques at the modeling stage. The issue lies within the feature engineering logic itself, which is likely producing extreme, non-physical values that cause the linear algebra to fail.

## Handoff
- **Stopping Point:** All reasonable attempts to stabilize the linear models have failed. The problem has been definitively isolated to the feature engineering code.
- **Next Immediate Task:** A manual, code-level audit and debugging of the feature generation pipeline in `src/features/core.py` is the only path forward. This should involve:
    - Adding extensive logging to trace the values of metrics at each stage of aggregation and adjustment.
    - Manually inspecting the output of each aggregation step for a small subset of games to find where extreme values are being introduced.
    - Considering more robust aggregation methods (e.g., using winsorization or clipping to handle outliers) within the feature engineering code itself.
- **Next Session Context:** Do not attempt any further model training. The next session must be a pure data quality and debugging session focused on `src/features/core.py`.

---
date: 2025-11-20
branch: main
task: [IMPL-task:MODEL-DEV] - Final attempt to stabilize spread model
---

# TL;DR (â‰¤5 lines)
- Made a final, multi-pronged attempt to stabilize the linear models by fixing `NaN` propagation in the feature engineering code, dropping `NaN` rows, pruning high-VIF features, and scaling the data.
- Despite these efforts, the `RuntimeWarning`s (divide by zero, overflow) still persisted, proving the root cause is a deep and subtle data quality issue.
- Concluded that the problem cannot be solved at the modeling stage and requires a manual, code-level audit of the feature engineering pipeline in `src/features/core.py`.
- All experimental scripts and configuration changes from this session have been reverted to leave the repository in a clean state.
- The key learning is that future modeling work is blocked until the data generation process itself is fixed.

**tags:** ["modeling", "spread-model", "feature-engineering", "failure-analysis", "numerical-instability", "data-quality"]

## Final Analysis
This session was a deep dive into a persistent numerical instability issue. The final, most robust attempt to fix it involved:
1.  **Fixing `NaN` propagation:** Corrected the opponent-adjustment logic in `src/features/core.py` to use `np.nansum`, preventing `NaN`s from spreading during the calculation.
2.  **Aggressive Data Cleaning:** Implemented a new training script that explicitly dropped any rows containing `NaN`s.
3.  **VIF Pruning:** The script also iteratively removed features with a Variance Inflation Factor (VIF) greater than 100.
4.  **Scaling:** The script used `StandardScaler` to normalize the feature set.

Even with all of these interventions, the `RuntimeWarning`s persisted. This proves that the problem is not addressable with standard data cleaning techniques at the modeling stage. The issue lies within the feature engineering logic itself, which is likely producing extreme, non-physical values that cause the linear algebra to fail.

## Handoff
- **Stopping Point:** All reasonable attempts to stabilize the linear models have failed. The problem has been definitively isolated to the feature engineering code.
- **Next Immediate Task:** A manual, code-level audit and debugging of the feature aggregation logic in `src/features/core.py` is the only path forward.
- **Recommendation for Next Session:**
    1.  Start with the `debug_feature_pipeline.py` script created in this session.
    2.  Instead of just checking for `inf` and `NaN` at the end of each function, add `print(df.describe().transpose())` and checks for `inf` *inside* the loops and calculations within `aggregate_team_game` and `aggregate_team_season`.
    3.  Trace the lineage of a single, problematic feature (like `home_off_rush_ypp_last_3`) from its raw components to its final value to see where the extreme values are introduced.
    4.  Once the source is found, implement a fix (e.g., clipping values, adding a larger epsilon, or reformulating the metric).
